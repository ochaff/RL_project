{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcd25b5",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tutorial-Reinforcement-Learning-with-Applications\" data-toc-modified-id=\"Tutorial-Reinforcement-Learning-with-Applications-1\">Tutorial Reinforcement Learning with Applications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Algorithmic-trading-tutorial-(optional)\" data-toc-modified-id=\"Algorithmic-trading-tutorial-(optional)-1.1\">Algorithmic trading tutorial (optional)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Work-plan\" data-toc-modified-id=\"Work-plan-1.1.1\">Work plan</a></span></li><li><span><a href=\"#Important-Remarks\" data-toc-modified-id=\"Important-Remarks-1.1.2\">Important Remarks</a></span></li></ul></li></ul></li><li><span><a href=\"#Tutorial-for-The-Algorithmic-Trading-Adventure:-Mastering-Deep-Reinforcement-Learning-in-Finance\" data-toc-modified-id=\"Tutorial-for-The-Algorithmic-Trading-Adventure:-Mastering-Deep-Reinforcement-Learning-in-Finance-2\">Tutorial for The Algorithmic Trading Adventure: Mastering Deep Reinforcement Learning in Finance</a></span></li><li><span><a href=\"#Part-1:-Reinforcement-Learning\" data-toc-modified-id=\"Part-1:-Reinforcement-Learning-3\">Part 1: Reinforcement Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-The-RL-Universe\" data-toc-modified-id=\"1.1-The-RL-Universe-3.1\">1.1 The RL Universe</a></span><ul class=\"toc-item\"><li><span><a href=\"#States\" data-toc-modified-id=\"States-3.1.1\">States</a></span></li></ul></li><li><span><a href=\"#State-Representation-in-a-Markov-Decision-Process\" data-toc-modified-id=\"State-Representation-in-a-Markov-Decision-Process-3.2\">State Representation in a Markov Decision Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Actions\" data-toc-modified-id=\"Actions-3.2.1\">Actions</a></span></li><li><span><a href=\"#Rewards\" data-toc-modified-id=\"Rewards-3.2.2\">Rewards</a></span></li><li><span><a href=\"#Markov-Decision-Processes-(MDPs)\" data-toc-modified-id=\"Markov-Decision-Processes-(MDPs)-3.2.3\">Markov Decision Processes (MDPs)</a></span><ul class=\"toc-item\"><li><span><a href=\"#State-Transition-Function\" data-toc-modified-id=\"State-Transition-Function-3.2.3.1\">State Transition Function</a></span></li><li><span><a href=\"#Reward-function\" data-toc-modified-id=\"Reward-function-3.2.3.2\">Reward function</a></span></li><li><span><a href=\"#Discount-Factor-(γ)\" data-toc-modified-id=\"Discount-Factor-(γ)-3.2.3.3\">Discount Factor (γ)</a></span></li></ul></li><li><span><a href=\"#Policies---Policy-function\" data-toc-modified-id=\"Policies---Policy-function-3.2.4\">Policies - Policy function</a></span></li><li><span><a href=\"#Value-Functions\" data-toc-modified-id=\"Value-Functions-3.2.5\">Value Functions</a></span></li><li><span><a href=\"#Summary:-RL\" data-toc-modified-id=\"Summary:-RL-3.2.6\">Summary: RL</a></span></li></ul></li><li><span><a href=\"#1.2-The-RL-Landscape\" data-toc-modified-id=\"1.2-The-RL-Landscape-3.3\">1.2 The RL Landscape</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definitions\" data-toc-modified-id=\"Definitions-3.3.1\">Definitions</a></span></li><li><span><a href=\"#Dynamic-Programming\" data-toc-modified-id=\"Dynamic-Programming-3.3.2\">Dynamic Programming</a></span></li><li><span><a href=\"#Monte-Carlo-Methods\" data-toc-modified-id=\"Monte-Carlo-Methods-3.3.3\">Monte Carlo Methods</a></span></li><li><span><a href=\"#Temporal-Difference-Learning\" data-toc-modified-id=\"Temporal-Difference-Learning-3.3.4\">Temporal Difference Learning</a></span></li></ul></li><li><span><a href=\"#1.3-Exercises:-Test-your-RL-skills\" data-toc-modified-id=\"1.3-Exercises:-Test-your-RL-skills-3.4\">1.3 Exercises: Test your RL skills</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-1\" data-toc-modified-id=\"Exercise-1-3.4.1\">Exercise 1</a></span></li><li><span><a href=\"#Exercise-2\" data-toc-modified-id=\"Exercise-2-3.4.2\">Exercise 2</a></span></li><li><span><a href=\"#Exercise-3\" data-toc-modified-id=\"Exercise-3-3.4.3\">Exercise 3</a></span></li><li><span><a href=\"#Exercise-4\" data-toc-modified-id=\"Exercise-4-3.4.4\">Exercise 4</a></span></li><li><span><a href=\"#Exercise-5\" data-toc-modified-id=\"Exercise-5-3.4.5\">Exercise 5</a></span></li></ul></li><li><span><a href=\"#1.4-Exercises:-Answers\" data-toc-modified-id=\"1.4-Exercises:-Answers-3.5\">1.4 Exercises: Answers</a></span></li><li><span><a href=\"#Exercise-1\" data-toc-modified-id=\"Exercise-1-3.6\">Exercise 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#State-Transition-Probabilities\" data-toc-modified-id=\"State-Transition-Probabilities-3.6.1\">State Transition Probabilities</a></span></li><li><span><a href=\"#Reward-Function\" data-toc-modified-id=\"Reward-Function-3.6.2\">Reward Function</a></span></li></ul></li></ul></li><li><span><a href=\"#Part-2:-Deep-Learning\" data-toc-modified-id=\"Part-2:-Deep-Learning-4\">Part 2: Deep Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definitions-with-Formulas\" data-toc-modified-id=\"Definitions-with-Formulas-4.1\">Definitions with Formulas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q-Value:\" data-toc-modified-id=\"Q-Value:-4.1.1\">Q-Value:</a></span></li><li><span><a href=\"#Q-Function:\" data-toc-modified-id=\"Q-Function:-4.1.2\">Q-Function:</a></span></li><li><span><a href=\"#Q-learning-Algorithm:\" data-toc-modified-id=\"Q-learning-Algorithm:-4.1.3\">Q-learning Algorithm:</a></span></li><li><span><a href=\"#Bellman-Equation:\" data-toc-modified-id=\"Bellman-Equation:-4.1.4\">Bellman Equation:</a></span></li></ul></li><li><span><a href=\"#Deep-Q-Networks-(DQNs)\" data-toc-modified-id=\"Deep-Q-Networks-(DQNs)-4.2\">Deep Q-Networks (DQNs)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network-Approximation:\" data-toc-modified-id=\"Neural-Network-Approximation:-4.2.1\">Neural Network Approximation:</a></span></li><li><span><a href=\"#Key-Features-of-DQNs:\" data-toc-modified-id=\"Key-Features-of-DQNs:-4.2.2\">Key Features of DQNs:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Experience-Replay:\" data-toc-modified-id=\"Experience-Replay:-4.2.2.1\">Experience Replay:</a></span></li><li><span><a href=\"#Target-Network:\" data-toc-modified-id=\"Target-Network:-4.2.2.2\">Target Network:</a></span></li></ul></li><li><span><a href=\"#Summary:\" data-toc-modified-id=\"Summary:-4.2.3\">Summary:</a></span></li></ul></li><li><span><a href=\"#2.1-Neural-Networks\" data-toc-modified-id=\"2.1-Neural-Networks-4.3\">2.1 Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Structure-of-Neural-Networks\" data-toc-modified-id=\"The-Structure-of-Neural-Networks-4.3.1\">The Structure of Neural Networks</a></span></li><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-4.3.2\">Activation Functions</a></span></li><li><span><a href=\"#Loss-Functions\" data-toc-modified-id=\"Loss-Functions-4.3.3\">Loss Functions</a></span></li><li><span><a href=\"#Optimization-Algorithms\" data-toc-modified-id=\"Optimization-Algorithms-4.3.4\">Optimization Algorithms</a></span></li></ul></li><li><span><a href=\"#2.2-Convolutional-Neural-Networks\" data-toc-modified-id=\"2.2-Convolutional-Neural-Networks-4.4\">2.2 Convolutional Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convolutional-Layers\" data-toc-modified-id=\"Convolutional-Layers-4.4.1\">Convolutional Layers</a></span></li><li><span><a href=\"#Pooling-Layers\" data-toc-modified-id=\"Pooling-Layers-4.4.2\">Pooling Layers</a></span></li><li><span><a href=\"#Fully-Connected-Layers\" data-toc-modified-id=\"Fully-Connected-Layers-4.4.3\">Fully Connected Layers</a></span></li></ul></li><li><span><a href=\"#2.3-Deep-Reinforcement-Learning\" data-toc-modified-id=\"2.3-Deep-Reinforcement-Learning-4.5\">2.3 Deep Reinforcement Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#DQN:-Deep-Q-Networks\" data-toc-modified-id=\"DQN:-Deep-Q-Networks-4.5.1\">DQN: Deep Q-Networks</a></span></li><li><span><a href=\"#Policy-Gradient-Methods:-Better-Policies\" data-toc-modified-id=\"Policy-Gradient-Methods:-Better-Policies-4.5.2\">Policy Gradient Methods: Better Policies</a></span></li><li><span><a href=\"#DRL-in-Finance\" data-toc-modified-id=\"DRL-in-Finance-4.5.3\">DRL in Finance</a></span></li></ul></li><li><span><a href=\"#2.4-Exercises:-Deep-Reinforcement-Learning\" data-toc-modified-id=\"2.4-Exercises:-Deep-Reinforcement-Learning-4.6\">2.4 Exercises: Deep Reinforcement Learning</a></span></li><li><span><a href=\"#2.5-Answers:-Validate-your-DRL-Knowledge\" data-toc-modified-id=\"2.5-Answers:-Validate-your-DRL-Knowledge-4.7\">2.5 Answers: Validate your DRL Knowledge</a></span></li></ul></li><li><span><a href=\"#Part-3:-Financial-Markets-with-DRL\" data-toc-modified-id=\"Part-3:-Financial-Markets-with-DRL-5\">Part 3: Financial Markets with DRL</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Building-the-Ultimate-Trading-Agent\" data-toc-modified-id=\"3.1-Building-the-Ultimate-Trading-Agent-5.1\">3.1 Building the Ultimate Trading Agent</a></span><ul class=\"toc-item\"><li><span><a href=\"#State-Representation:-Decoding-Markes\" data-toc-modified-id=\"State-Representation:-Decoding-Markes-5.1.1\">State Representation: Decoding Markes</a></span></li><li><span><a href=\"#Action-Space:-Trading\" data-toc-modified-id=\"Action-Space:-Trading-5.1.2\">Action Space: Trading</a></span></li><li><span><a href=\"#Reward-Function\" data-toc-modified-id=\"Reward-Function-5.1.3\">Reward Function</a></span></li></ul></li><li><span><a href=\"#3.2-DRL-Algorithms-for-Finance\" data-toc-modified-id=\"3.2-DRL-Algorithms-for-Finance-5.2\">3.2 DRL Algorithms for Finance</a></span><ul class=\"toc-item\"><li><span><a href=\"#DQN:-The-Deep-Q-Network-Pioneer\" data-toc-modified-id=\"DQN:-The-Deep-Q-Network-Pioneer-5.2.1\">DQN: The Deep Q-Network Pioneer</a></span></li><li><span><a href=\"#Policy-Gradient-Methods\" data-toc-modified-id=\"Policy-Gradient-Methods-5.2.2\">Policy Gradient Methods</a></span></li><li><span><a href=\"#Actor-Critic-Methods:-The-Best-of-Both-Worlds\" data-toc-modified-id=\"Actor-Critic-Methods:-The-Best-of-Both-Worlds-5.2.3\">Actor-Critic Methods: The Best of Both Worlds</a></span></li></ul></li><li><span><a href=\"#3.3-Implementing-a-DRL-Trading-Agent\" data-toc-modified-id=\"3.3-Implementing-a-DRL-Trading-Agent-5.3\">3.3 Implementing a DRL Trading Agent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Data-Preparation\" data-toc-modified-id=\"Step-1:-Data-Preparation-5.3.1\">Step 1: Data Preparation</a></span></li><li><span><a href=\"#Step-2:-State-Representation-and-Action-Space\" data-toc-modified-id=\"Step-2:-State-Representation-and-Action-Space-5.3.2\">Step 2: State Representation and Action Space</a></span></li><li><span><a href=\"#Step-3:-Algorithm-Selection\" data-toc-modified-id=\"Step-3:-Algorithm-Selection-5.3.3\">Step 3: Algorithm Selection</a></span></li><li><span><a href=\"#Step-4:-Model-Implementation\" data-toc-modified-id=\"Step-4:-Model-Implementation-5.3.4\">Step 4: Model Implementation</a></span></li><li><span><a href=\"#Step-5:-Reward-Function-and-Training\" data-toc-modified-id=\"Step-5:-Reward-Function-and-Training-5.3.5\">Step 5: Reward Function and Training</a></span></li><li><span><a href=\"#Step-6:-Evaluation-and-Backtesting\" data-toc-modified-id=\"Step-6:-Evaluation-and-Backtesting-5.3.6\">Step 6: Evaluation and Backtesting</a></span></li><li><span><a href=\"#Step-7:-Refinement-and-Deployment\" data-toc-modified-id=\"Step-7:-Refinement-and-Deployment-5.3.7\">Step 7: Refinement and Deployment</a></span></li></ul></li><li><span><a href=\"#3.4-Exercises\" data-toc-modified-id=\"3.4-Exercises-5.4\">3.4 Exercises</a></span></li><li><span><a href=\"#3.5-Answers\" data-toc-modified-id=\"3.5-Answers-5.5\">3.5 Answers</a></span></li></ul></li><li><span><a href=\"#Part-4:-Portfolio-Optimization-and-Risk-Management:-The-DRL-Way\" data-toc-modified-id=\"Part-4:-Portfolio-Optimization-and-Risk-Management:-The-DRL-Way-6\">Part 4: Portfolio Optimization and Risk Management: The DRL Way</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Modern-Portfolio-Theory:-The-Classic-Approach\" data-toc-modified-id=\"4.1-Modern-Portfolio-Theory:-The-Classic-Approach-6.1\">4.1 Modern Portfolio Theory: The Classic Approach</a></span></li><li><span><a href=\"#4.2-DRL:-Overcoming-MPT-Limitations\" data-toc-modified-id=\"4.2-DRL:-Overcoming-MPT-Limitations-6.2\">4.2 DRL: Overcoming MPT Limitations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Nonlinear-Relationships-and-Complex-Market-Dynamics\" data-toc-modified-id=\"Nonlinear-Relationships-and-Complex-Market-Dynamics-6.2.1\">Nonlinear Relationships and Complex Market Dynamics</a></span></li><li><span><a href=\"#Continuous-Action-Spaces\" data-toc-modified-id=\"Continuous-Action-Spaces-6.2.2\">Continuous Action Spaces</a></span></li><li><span><a href=\"#Risk-sensitive-Exploration-and-Exploitation\" data-toc-modified-id=\"Risk-sensitive-Exploration-and-Exploitation-6.2.3\">Risk-sensitive Exploration and Exploitation</a></span></li><li><span><a href=\"#Online-Learning-and-Adaptation\" data-toc-modified-id=\"Online-Learning-and-Adaptation-6.2.4\">Online Learning and Adaptation</a></span></li></ul></li><li><span><a href=\"#4.3-A-DRL-Portfolio-Optimization-Agent\" data-toc-modified-id=\"4.3-A-DRL-Portfolio-Optimization-Agent-6.3\">4.3 A DRL Portfolio Optimization Agent</a></span><ul class=\"toc-item\"><li><span><a href=\"#State-Representation\" data-toc-modified-id=\"State-Representation-6.3.1\">State Representation</a></span></li><li><span><a href=\"#Action-Space\" data-toc-modified-id=\"Action-Space-6.3.2\">Action Space</a></span></li><li><span><a href=\"#Reward-Function\" data-toc-modified-id=\"Reward-Function-6.3.3\">Reward Function</a></span></li><li><span><a href=\"#DRL-Algorithm-Selection\" data-toc-modified-id=\"DRL-Algorithm-Selection-6.3.4\">DRL Algorithm Selection</a></span></li><li><span><a href=\"#Model-Implementation-and-Training\" data-toc-modified-id=\"Model-Implementation-and-Training-6.3.5\">Model Implementation and Training</a></span></li><li><span><a href=\"#Evaluation-and-Refinement\" data-toc-modified-id=\"Evaluation-and-Refinement-6.3.6\">Evaluation and Refinement</a></span></li></ul></li><li><span><a href=\"#4.4-Exercises:-DRL-Portfolio\" data-toc-modified-id=\"4.4-Exercises:-DRL-Portfolio-6.4\">4.4 Exercises: DRL Portfolio</a></span></li><li><span><a href=\"#4.5-Answers:-Validate-your-DRL-Portfolio-Knowledge\" data-toc-modified-id=\"4.5-Answers:-Validate-your-DRL-Portfolio-Knowledge-6.5\">4.5 Answers: Validate your DRL Portfolio Knowledge</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05e367-83f2-441c-bb09-1c3af413c4ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial Reinforcement Learning with Applications\n",
    "## Algorithmic trading tutorial (optional)\n",
    "\n",
    "This optional tutorial helps you to prepare for the Algorithmic Trading project. The tutorial provides a stepwise guide through various tasks and concepts that can be reused in the project. Completing this tutorial is not mandatory and the results are not evaluated, but you are recommended to go through it in parallel with the Algorithmic Trading project, as it provides starting code, facilitates understanding and provides guidance in completing the project tasks. The following remarks apply, catering to the individual requirements and speed of each group.\n",
    "\n",
    "### Work plan\n",
    "* (Optional) Try to complete the tutorial within the first 3 weeks of the course.\n",
    "* (Recommended) Start with the project and consult this tutorial as needed and in parallel.\n",
    "* (Recommended) Always consult the tutorial in parallel if and when needed.\n",
    "* (Recommended) Make use of office hours that are offered at any time.\n",
    "\n",
    "### Important Remarks\n",
    "* The tutorial contains many useful bits and pieces needed to complete the project.\n",
    "* Functions are strongly preferred over using classes for this exercise.\n",
    "* Part 0 to 4 are useful to consult if any tasks are unclear.\n",
    "* Part 4 shows similar tasks applied to portfolio optimization instead of stock trading and can serve as some additional guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f5134",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "<a name=\"\"></a> \n",
    "# Tutorial for The Algorithmic Trading Adventure: Mastering Deep Reinforcement Learning in Finance\n",
    "\n",
    "Welcome to the Algorithmic Trading Adventure! You will embark on a journey to explore the world of deep reinforcement learning (DRL) and its applications in finance. By the end of this course, you'll be equipped with the knowledge and tools to create your very own DRL-powered trading strategies.\n",
    "\n",
    "Together with the source code, this tutorial will serve as your interactive guide throughout the course. You'll find code snippets and explanations to help you understand and implement the concepts covered in each module.\n",
    "\n",
    "Let's get started by setting up our environment and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb19a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:00:03.963812Z",
     "start_time": "2023-04-16T18:59:59.678437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for visualizations\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Enable inline plotting for matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944c485",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Now that our environment is set up, we're ready to dive into the world of deep reinforcement learning and uncover its potential in the financial industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213399e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Part 1: Reinforcement Learning \n",
    "\n",
    "Welcome to the first module of our journey! In this module, we'll explore the fascinating world of reinforcement learning. We'll begin by unveiling the core concepts of the RL universe.\n",
    "\n",
    "## 1.1 The RL Universe\n",
    "\n",
    "Reinforcement learning is an area of machine learning that enables agents to learn how to make decisions based on interactions with their environment. At the heart of the RL universe, there are several key components we must understand:\n",
    "\n",
    "- States\n",
    "- Actions\n",
    "- Rewards\n",
    "- Markov Decision Processes (MDPs)\n",
    "- Policies\n",
    "- Value functions\n",
    "\n",
    "Let's dive into each of these components and discover their roles in RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05718336",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "In RL, a state represents a specific situation or configuration in the environment. For instance, in a stock market, the state could include information like current stock prices, trading volume, and technical indicators.\n",
    "\n",
    "In this course, we'll often represent states as vectors or matrices. Let's create a simple example of a state using NumPy.\n",
    "\n",
    "## State Representation in a Markov Decision Process\n",
    "\n",
    "In many Markov Decision Process (MDP) models, particularly those in financial trading scenarios, the state is often represented as an array of continuous values. Each element of this array provides critical information that affects decision-making:\n",
    "\n",
    "- **state[0]: Current Price of an Asset**\n",
    "  - This value represents the most recent trading price of a specific asset. It is crucial for making immediate buy or sell decisions.\n",
    "\n",
    "- **state[1]: Highest Price of the Asset in the Last Period**\n",
    "  - Monitoring the highest price within a specific time frame helps in assessing volatility and potential price ceilings.\n",
    "\n",
    "- **state[2]: Moving Average of the Asset Price**\n",
    "  - The moving average smooths out price data by creating a constantly updated average price, which is beneficial for identifying trends.\n",
    "\n",
    "- **state[3]: Volume of the Asset Being Traded**\n",
    "  - Trading volume indicates the total quantity of an asset that was traded in a given time period and is a measure of the asset's liquidity, often used to confirm trends or signals derived from price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bbdde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T18:41:05.611650Z",
     "start_time": "2023-04-16T18:41:05.589978Z"
    }
   },
   "outputs": [],
   "source": [
    "state = np.array([100, 110, 0.5, 200])\n",
    "print(\"State:\", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19365a39",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "An action is a decision made by the agent to interact with the environment. In the context of finance, an action could represent buying, selling, or holding a stock. We can represent actions as integers, where each integer corresponds to a specific action.\n",
    "\n",
    "For example, let's create a dictionary to map action integers to their meanings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a6ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T18:41:23.554154Z",
     "start_time": "2023-04-16T18:41:23.532867Z"
    }
   },
   "outputs": [],
   "source": [
    "actions = {0: 'Buy', 1: 'Sell', 2: 'Hold'}\n",
    "print(\"Actions:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16060a3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ef2022d",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "A reward is a scalar value that the agent receives after taking an action in a given state. The goal of the agent is to learn a strategy that maximizes the cumulative reward over time.\n",
    "\n",
    "In finance, the reward could be the profit or loss resulting from a trade. Let's create a simple example of a reward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e10328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T18:41:45.845013Z",
     "start_time": "2023-04-16T18:41:45.826299Z"
    }
   },
   "outputs": [],
   "source": [
    "reward = 20  # Profit from a trade\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633c89c",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (MDPs)\n",
    "\n",
    "A Markov Decision Process (MDP) is a mathematical framework used to model decision-making problems in RL. An MDP is defined by the following components:\n",
    "\n",
    "- A set of States (S)\n",
    "- A set of Actions (A)\n",
    "- A state transition function (P)\n",
    "- A reward function (R)\n",
    "- A discount factor (γ)\n",
    "\n",
    "MDPs assume that the future state depends only on the current state and action, and not on the history of previous states and actions. This is known as the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d42193",
   "metadata": {},
   "source": [
    "#### State Transition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the state and actions\n",
    "state = np.array([100, 110, 0.5, 200])  # Example state\n",
    "actions = {0: 'Buy', 1: 'Sell', 2: 'Hold'}  # Actions dictionary\n",
    "\n",
    "# Simple state transition function\n",
    "def state_transition_fct(current_state, action_key):\n",
    "    \"\"\"\n",
    "    Simulates the next state based on the current state and the action taken.\n",
    "    For simplicity:\n",
    "    - 'Buy' increases the price of the asset by 10.\n",
    "    - 'Sell' decreases the price of the asset by 10.\n",
    "    - 'Hold' leaves the state unchanged.\n",
    "    \"\"\"\n",
    "    new_state = current_state.copy()\n",
    "    if action_key == 0:  # Buy\n",
    "        new_state[0] += 10\n",
    "    elif action_key == 1:  # Sell\n",
    "        new_state[0] -= 10\n",
    "    # 'Hold' action does not change the state\n",
    "    return new_state\n",
    "\n",
    "# Example usage\n",
    "current_action_key = 0  # 'Buy'\n",
    "new_state = state_transition_fct(state, current_action_key)\n",
    "print(\"New State after 'Buy':\", new_state)\n",
    "\n",
    "current_action_key = 1  # 'Sell'\n",
    "new_state = state_transition_fct(state, current_action_key)\n",
    "print(\"New State after 'Sell':\", new_state)\n",
    "\n",
    "current_action_key = 2  # 'Hold'\n",
    "new_state = state_transition_fct(state, current_action_key)\n",
    "print(\"New State after 'Hold':\", new_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a8885",
   "metadata": {},
   "source": [
    "#### Reward function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fe6c9",
   "metadata": {},
   "source": [
    "##### Reward Function Definition:\n",
    "\n",
    "This function considers both the action taken and the changes in the market conditions as represented by the state transitions:\n",
    "\n",
    "- **Inputs**:\n",
    "  - `current_state`: The state of the market before the action is taken.\n",
    "  - `next_state`: The state of the market after the action is taken.\n",
    "  - `action`: The action performed by the agent.\n",
    "\n",
    "- **State Elements**:\n",
    "  - `state[0]`: Current price of the asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Python Example:\n",
    "\n",
    "def reward_function(current_state, next_state, action):\n",
    "    # Extract relevant elements\n",
    "    current_price = current_state[0]\n",
    "    next_price = next_state[0]\n",
    "\n",
    "    if action == 'Buy':\n",
    "        # Reward is calculated based on the future price increase\n",
    "        reward = next_price - current_price\n",
    "    elif action == 'Sell':\n",
    "        # Reward for selling is based on the price increase since purchase\n",
    "        reward = current_price - next_price \n",
    "    else:  # 'Hold'\n",
    "        # Minimal reward or penalty for holding, depending on the market volatility\n",
    "        reward = 0\n",
    "        \n",
    "    return reward\n",
    "\n",
    "# Example usage:\n",
    "current_state = [100, 110, 0.5, 200]\n",
    "next_state = [105, 115, 0.55, 210]\n",
    "action = 'Buy'\n",
    "reward = reward_function(current_state, next_state, action)\n",
    "print(\"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700774b4",
   "metadata": {},
   "source": [
    "#### Discount Factor (γ) \n",
    "\n",
    "##### Definition:\n",
    "The discount factor, \\( \\gamma \\), is a value between 0 and 1 that weighs the importance of future rewards in reinforcement learning algorithms. It is used to calculate the present value of expected future rewards.\n",
    "\n",
    "##### Importance:\n",
    "- **\\( \\gamma = 0 \\)**: The agent values only immediate rewards, focusing on short-term gains.\n",
    "- **\\( \\gamma = 1 \\)**: The agent considers future rewards just as important as immediate rewards, aiming for long-term benefits.\n",
    "\n",
    "##### Usage:\n",
    "The discount factor is applied in the calculation of the expected cumulative reward, influencing the agent's policy by balancing between immediate and future rewards. It helps in stabilizing the learning and decision-making process, particularly in environments with varying reward structures and time horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [10, 20, 30]  # List of rewards\n",
    "gamma = 0.9  # Discount factor\n",
    "discounted_reward = sum(reward * (gamma ** i) for i, reward in enumerate(rewards))\n",
    "print(\"Discounted Reward:\", discounted_reward)\n",
    "# also sometimes called return, or expected return. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362738d",
   "metadata": {},
   "source": [
    "### Policies - Policy function\n",
    "\n",
    "A policy is a function that maps states to actions. It represents the agent's strategy or decision-making process. The goal of RL is to learn an optimal policy that maximizes the cumulative reward over time.\n",
    "\n",
    "We can represent a policy as a Python function or a table. For simplicity, let's create a naive policy that always buys stocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22806b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T18:42:25.662349Z",
     "start_time": "2023-04-16T18:42:25.640763Z"
    }
   },
   "outputs": [],
   "source": [
    "actions = {0: 'Buy', 1: 'Sell', 2: 'Hold'}\n",
    "def naive_policy(state):\n",
    "    return actions[0]  # Always buy\n",
    "\n",
    "example_state = np.array([100, 110, 0.5, 200])\n",
    "action = naive_policy(example_state)\n",
    "print(f\"Action for example state {example_state}: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f3f86",
   "metadata": {},
   "source": [
    "### Value Functions\n",
    "\n",
    "Value functions are used to estimate the expected cumulative reward for a given state or state-action pair. There are two main types of value functions:\n",
    "\n",
    "- State-value function (V): Represents the expected cumulative reward for a given state, following a specific policy.\n",
    "- Action-value function (Q): Represents the expected cumulative reward for a given state and action, following a specific policy.\n",
    "\n",
    "Value functions are crucial in reinforcement learning, as they help the agent evaluate the goodness of states and actions. We can represent value functions as tables, neural networks, or other function approximators.\n",
    "\n",
    "For simplicity, let's create a basic value function that returns a random value for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946d543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T18:43:50.425708Z",
     "start_time": "2023-04-16T18:43:50.414222Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_value_function(state):\n",
    "    return np.random.rand()\n",
    "\n",
    "value = random_value_function(example_state)\n",
    "print(f\"Value for example state {example_state}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0cd0d",
   "metadata": {},
   "source": [
    "### Summary: RL\n",
    "\n",
    "Reinforcement Learning (RL) involves a set of core concepts that define its framework, crucial for modeling decision-making processes:\n",
    "\n",
    "1. **Sets**:\n",
    "   - **States**: Possible configurations or conditions the agent can observe in the environment.\n",
    "   - **Actions**: Set of possible decisions or moves the agent can take.\n",
    "   - **Rewards**: Feedback received after taking actions, guiding the agent's learning.\n",
    "\n",
    "2. **Functions**:\n",
    "   - **2.1 Policy Function**: Maps a state to an action. It dictates the agent's behavior at each state by choosing an action based on the current policy.\n",
    "   - **2.2 Value Function**: Assigns a real number to a state or a state-action pair, representing the expected return starting from that state or taking an action in that state, following a particular policy.\n",
    "   - **2.3 Reward Function**: Provides a real number reward for each state-action pair, indicating the immediate return of taking a specific action in a given state.\n",
    "\n",
    "These elements collectively form the backbone of any RL system, facilitating the agent's ability to learn optimal behaviors through interaction with its environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the state array representing financial metrics\n",
    "state = np.array([100, 110, 0.5, 200])  # [current price, high price, moving average, trading volume]\n",
    "\n",
    "# Map integer keys to trading actions\n",
    "actions = {0: 'Buy', 1: 'Sell', 2: 'Hold'}\n",
    "\n",
    "# Set a constant reward for simplification\n",
    "reward = 20  # Hypothetical profit from a trade\n",
    "\n",
    "def naive_policy(state):\n",
    "    \"\"\"A simplistic policy that always decides to buy, regardless of the state.\"\"\"\n",
    "    return actions[0]  # Always returns 'Buy'\n",
    "\n",
    "def random_value_function(state):\n",
    "    \"\"\"Generates a random value for a given state, simulating unpredictability in evaluation.\"\"\"\n",
    "    return np.random.rand()  # Returns a random float between 0 and 1\n",
    "\n",
    "def reward_function(state, action):\n",
    "    \"\"\"Calculates a random reward, mimicking variable outcomes of trading actions.\"\"\"\n",
    "    if action == 'Buy':\n",
    "        return np.random.rand() * 100  # Random reward up to 100 for buying\n",
    "    elif action == 'Sell':\n",
    "        return np.random.rand() * 200  # Random reward up to 200 for selling\n",
    "    else:\n",
    "        return 0  # No reward for holding\n",
    "\n",
    "# Example usage:\n",
    "action_taken = naive_policy(state)\n",
    "value_estimate = random_value_function(state)\n",
    "trade_reward = reward_function(state, action_taken)\n",
    "\n",
    "print(f\"Action Taken: {action_taken}, Value Estimate: {value_estimate:.2f}, Trade Reward: {trade_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea15bb7",
   "metadata": {},
   "source": [
    "With the core concepts of reinforcement learning introduced, we're now prepared to explore the vast landscape of RL algorithms and techniques. In the next section, we'll learn about some fundamental RL algorithms that serve as the foundation for deep reinforcement learning.\n",
    "\n",
    "Proceed to Section 1.2: **Conquering the RL Landscape**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaa58b",
   "metadata": {},
   "source": [
    "## 1.2 The RL Landscape\n",
    "\n",
    "In this section, we'll explore some fundamental reinforcement learning algorithms that serve as the foundation for more advanced deep reinforcement learning techniques. We'll cover three main approaches:\n",
    "\n",
    "1. Dynamic Programming\n",
    "2. Monte Carlo Methods\n",
    "3. Temporal Difference Learning\n",
    "\n",
    "By understanding these foundational algorithms, you'll be better prepared to master the deep reinforcement learning techniques used in finance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fcb3b",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "Model of the environment: state transition function, reward function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44dea06",
   "metadata": {},
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "Dynamic programming (DP) is a family of algorithms used to solve MDPs when the model of the environment (i.e., state transition function and reward function) is known. DP algorithms are based on the principles of optimality and value iteration.\n",
    "\n",
    "There are two main DP algorithms:\n",
    "\n",
    "1. Value Iteration\n",
    "2. Policy Iteration\n",
    "\n",
    "Both algorithms aim to find the optimal policy and value function for a given MDP. However, their approaches differ: Value Iteration focuses on iteratively updating the value function, while Policy Iteration alternates between policy evaluation and policy improvement steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621c3a8",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods\n",
    "\n",
    "Monte Carlo (MC) methods are a class of reinforcement learning algorithms that learn from actual experiences or simulations. They rely on averaging over multiple sample episodes to estimate value functions and improve the policy.\n",
    "\n",
    "MC methods are particularly useful when the environment model is not known, as they don't require knowledge of the state transition function or reward function. Instead, they learn directly from the agent's interactions with the environment.\n",
    "\n",
    "Some key characteristics of MC methods are:\n",
    "\n",
    "1. They are model-free: They don't require knowledge of the model environment.\n",
    "2. They are episodic: They learn from complete episodes or sequences of interactions.\n",
    "3. They use sample-based estimates: They average the returns of multiple episodes to estimate value functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fc570",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning\n",
    "\n",
    "Temporal Difference (TD) learning is another class of model-free reinforcement learning algorithms. It combines elements of both Dynamic Programming and Monte Carlo methods to learn value functions and policies from the agent's experiences.\n",
    "\n",
    "TD methods update value estimates based on the difference between the current and next state values, called the temporal difference. This approach allows TD methods to learn online, without waiting for the episode to complete, unlike Monte Carlo methods.\n",
    "\n",
    "There are several popular TD algorithms:\n",
    "\n",
    "1. TD-learning: A state-value function learning method.\n",
    "2. Q-learning: An off-policy action-value function learning method.\n",
    "3. SARSA: An on-policy action-value function learning method.\n",
    "\n",
    "These algorithms form the foundation of many advanced reinforcement learning techniques, including deep reinforcement learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c4295",
   "metadata": {},
   "source": [
    "## 1.3 Exercises: Test your RL skills\n",
    "\n",
    "Now that we've covered the fundamentals of reinforcement learning, it's time to put your knowledge to the test. Solve the following exercises to gain a deeper understanding of the concepts we've discussed in this module.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Consider a simple MDP with the following states and actions:\n",
    "\n",
    "- States: S = {s1, s2, s3}\n",
    "- Actions: A = {a1, a2}\n",
    "\n",
    "Write down the state transition function P(s, a, s') and the reward function R(s, a) for this MDP. You can choose any probabilities and rewards, but make sure the probabilities for each state and action sum to 1.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Implement a Python function to represent the policy for the MDP described in Exercise 1. Your function should take a state as input and return an action. You can choose any policy you like (e.g., always take action a1, alternate between actions, etc.).\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Using the MDP from Exercise 1, calculate the value function V(s) for each state following your policy from Exercise 2. Assume a discount factor γ of 0.9. You can calculate the value function by hand or by implementing a Python function.\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "What are the main differences between Monte Carlo methods and Temporal Difference learning algorithms? Provide at least three key differences.\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Explain the main differences between Q-learning and SARSA algorithms. Which one is off-policy, and which one is on-policy? What are the implications of this difference in a trading context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93225191",
   "metadata": {},
   "source": [
    "## 1.4 Exercises: Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fafef8",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "This exercise involves understanding the dynamics defined by state transition probabilities and corresponding rewards for actions in each state. Given below are the state transition function \\( P(s, a, s') \\) and reward function \\( R(s, a) \\).\n",
    "\n",
    "### State Transition Probabilities\n",
    "\n",
    "- For state \\( s1 \\):\n",
    "  - Action \\( a1 \\):\n",
    "    - \\( P(s1, a1, s1) = 0.6 \\)\n",
    "    - \\( P(s1, a1, s2) = 0.4 \\)\n",
    "    - \\( P(s1, a1, s3) = 0.0 \\)\n",
    "  - Action \\( a2 \\):\n",
    "    - \\( P(s1, a2, s1) = 0.2 \\)\n",
    "    - \\( P(s1, a2, s2) = 0.8 \\)\n",
    "    - \\( P(s1, a2, s3) = 0.0 \\)\n",
    "\n",
    "- For state \\( s2 \\):\n",
    "  - Action \\( a1 \\):\n",
    "    - \\( P(s2, a1, s1) = 0.0 \\)\n",
    "    - \\( P(s2, a1, s2) = 0.5 \\)\n",
    "    - \\( P(s2, a1, s3) = 0.5 \\)\n",
    "  - Action \\( a2 \\):\n",
    "    - \\( P(s2, a2, s1) = 0.1 \\)\n",
    "    - \\( P(s2, a2, s2) = 0.9 \\)\n",
    "    - \\( P(s2, a2, s3) = 0.0 \\)\n",
    "\n",
    "- For state \\( s3 \\):\n",
    "  - Action \\( a1 \\):\n",
    "    - \\( P(s3, a1, s1) = 0.2 \\)\n",
    "    - \\( P(s3, a1, s2) = 0.0 \\)\n",
    "    - \\( P(s3, a1, s3) = 0.8 \\)\n",
    "  - Action \\( a2 \\):\n",
    "    - \\( P(s3, a2, s1) = 0.0 \\)\n",
    "    - \\( P(s3, a2, s2) = 0.1 \\)\n",
    "    - \\( P(s3, a2, s3) = 0.9 \\)\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "- For state \\( s1 \\):\n",
    "  - \\( R(s1, a1) = 10 \\)\n",
    "  - \\( R(s1, a2) = -5 \\)\n",
    "\n",
    "- For state \\( s2 \\):\n",
    "  - \\( R(s2, a1) = -20 \\)\n",
    "  - \\( R(s2, a2) = 30 \\)\n",
    "\n",
    "- For state \\( s3 \\):\n",
    "  - \\( R(s3, a1) = 5 \\)\n",
    "  - \\( R(s3, a2) = 15 \\)\n",
    "\n",
    "This setup provides the basis for understanding how decisions affect transitions between states and the associated rewards, critical for planning and policy development in reinforcement learning scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a636158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2\n",
    "#Here's a simple policy function that always takes action a1:\n",
    "def policy(state):\n",
    "    return 'a1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d7ad8",
   "metadata": {},
   "source": [
    "ToDO Solution to Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98866dee",
   "metadata": {},
   "source": [
    "Exercise 4\n",
    "The main differences between Monte Carlo methods and Temporal Difference learning algorithms are:\n",
    "\n",
    "Monte Carlo methods are episodic and learn from complete episodes, whereas TD methods learn online and update value estimates during an episode.\n",
    "Monte Carlo methods use sample-based estimates, averaging the returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9bafb6",
   "metadata": {},
   "source": [
    "ToDO Solution to Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dd582",
   "metadata": {},
   "source": [
    "With a solid understanding of these foundational reinforcement learning algorithms, you're now prepared to embark on the deep reinforcement learning adventure. In the next module, we'll explore the world of deep learning and its core concepts.\n",
    "\n",
    "Proceed to Module 2: **The Deep Learning Expedition**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e1e2a",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Part 2: Deep Learning\n",
    "\n",
    "Welcome to the second module of our course! In this module, we'll go into the world of deep learning and explore the mysterious yet powerful world of neural networks. We'll cover:\n",
    "\n",
    "- The structure of neural networks\n",
    "- Activation functions\n",
    "- Loss functions\n",
    "- Optimization algorithms\n",
    "\n",
    "With a solid foundation in deep learning, you'll be prepared to combine it with reinforcement learning techniques to analyze financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab3a6e",
   "metadata": {},
   "source": [
    "## Definitions with Formulas\n",
    "\n",
    "In reinforcement learning (RL), understanding the mathematical foundations is essential for effectively implementing algorithms that enable agents to learn optimal behaviors. Below are the definitions of key concepts, accompanied by their respective formulas:\n",
    "\n",
    "### Q-Value:\n",
    "- **Definition**: The Q-value or action-value, quantifies the expected utility of taking a given action \\( a \\) in a given state \\( s \\), following a specific policy. \n",
    "- **Formula**:\n",
    "  \\[\n",
    "  Q(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') \\mid S_t = s, A_t = a \\right]\n",
    "  \\]\n",
    "\n",
    "### Q-Function:\n",
    "- **Definition**: The Q-function, or Q-table, maps state-action pairs to their respective Q-values and is updated based on experiences.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a) \\right]\n",
    "  \\]\n",
    "\n",
    "### Q-learning Algorithm:\n",
    "- **Definition**: Q-learning is an off-policy learner that updates the Q-values using the Bellman equation independent of the policy being followed by the agent.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') \\right]\n",
    "  \\]\n",
    "  Where \\( R \\) is the reward received after taking action \\( a \\) in state \\( s \\), \\( s' \\) is the subsequent state, and \\( \\gamma \\) is the discount factor.\n",
    "\n",
    "### Bellman Equation:\n",
    "- **Definition**: The Bellman equation recursively defines the relationship between the current and future Q-values.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  Q(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\max_{a'} Q(s', a')\n",
    "  \\]\n",
    "  Where \\( R(s, a) \\) is the immediate reward for taking action \\( a \\) in state \\( s \\), \\( P(s' \\mid s, a) \\) is the probability of reaching state \\( s' \\) from state \\( s \\) by taking action \\( a \\), and \\( \\max_{a'} Q(s', a') \\) is the maximum Q-value achievable from the next state \\( s' \\).\n",
    "\n",
    "These definitions and formulas provide a fundamental framework for developing and analyzing reinforcement learning algorithms, guiding the prediction and optimization of agent actions to maximize cumulative rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13d7c7",
   "metadata": {},
   "source": [
    "## Deep Q-Networks (DQNs)\n",
    "\n",
    "Deep Q-Networks (DQNs) enhance traditional Q-learning by using a neural network to approximate the Q-function. This approach maps state-action pairs to Q-values, significantly increasing the efficiency and scalability of learning in complex environments.\n",
    "\n",
    "### Neural Network Approximation:\n",
    "- **Functionality**: The neural network in a DQN approximates the Q-function, effectively mapping state-action pairs to their respective Q-values.\n",
    "- **Training Objective**: During training, the network is trained to minimize the loss, which is the difference between predicted Q-values and the target Q-values derived from the Bellman equation. This process ensures that the network learns to predict more accurate Q-values over time.\n",
    "\n",
    "### Key Features of DQNs:\n",
    "\n",
    "#### Experience Replay:\n",
    "- **Purpose**: DQNs utilize a replay buffer that stores tuples of experiences, including state, action, reward, and the subsequent state encountered after the action.\n",
    "- **Benefits**:\n",
    "  - **Random Sampling**: By sampling batches of experiences randomly from the replay buffer, DQNs break the correlation between consecutive training samples. This randomness is crucial for stabilizing the learning process as it prevents the network from overfitting to recent experiences and reduces the variance of updates.\n",
    "  - **Efficient Use of Past Experiences**: This technique allows the network to learn from individual experiences multiple times, maximizing the informational value of each experience and improving learning efficiency.\n",
    "\n",
    "#### Target Network:\n",
    "- **Functionality**: DQNs employ a secondary network, known as the target network, which is used to generate the target Q-values for training the primary network.\n",
    "- **Configuration**:\n",
    "  - **Architecture**: The target network has the same architecture as the primary (online) network but its weights are updated less frequently.\n",
    "  - **Stabilization**: This delayed update helps stabilize the learning process by providing consistent targets for the primary network to aim for during training intervals. The less frequent updates prevent the rapid propagation of errors that might arise from constantly shifting Q-value estimations.\n",
    "\n",
    "### Summary:\n",
    "Deep Q-Networks represent a significant advancement in reinforcement learning, offering a robust framework for handling high-dimensional, complex decision-making tasks. The integration of neural network approximations, experience replay, and the use of a target network collectively contribute to the efficacy and stability of the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a0b11",
   "metadata": {},
   "source": [
    "## 2.1 Neural Networks\n",
    "\n",
    "Neural networks are a class of machine learning models inspired by the structure and function of the human brain. They consist of interconnected layers of neurons that work together to learn complex patterns from data. In this section, we'll explore the key components of neural networks and the role each plays in the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e7331",
   "metadata": {},
   "source": [
    "### The Structure of Neural Networks\n",
    "\n",
    "A neural network typically consists of an input layer, one or more hidden layers, and an output layer. Each layer contains neurons that process and transmit information to the next layer.\n",
    "\n",
    "- **Input layer**: The input layer receives data from external sources and passes it to the first hidden layer.\n",
    "- **Hidden layers**: Hidden layers perform the majority of the processing in the neural network. They extract and learn features from the input data. Increasing the number of hidden layers can help the network learn more complex patterns.\n",
    "- **Output layer**: The output layer produces the final predictions or decisions of the neural network.\n",
    "\n",
    "Each neuron in a layer is connected to all neurons in the previous and next layers. These connections are assigned weights, which the neural network learns during training. Neurons also have bias terms, which help shift the activation functions and improve the network's ability to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4c8db",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions are mathematical functions applied to the output of each neuron. They introduce non-linearity into the neural network, enabling it to learn complex patterns and relationships. Some common activation functions include:\n",
    "\n",
    "- Sigmoid\n",
    "- Hyperbolic tangent (tanh)\n",
    "- Rectified linear unit (ReLU)\n",
    "- Leaky ReLU\n",
    "- Exponential linear unit (ELU)\n",
    "\n",
    "Each activation function has its unique characteristics and is suitable for different types of problems. It's important to choose the right activation function for the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0695d",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "Loss functions, also known as cost functions or objective functions, measure the difference between the neural network's predictions and the true target values. During training, the goal is to minimize the loss function to improve the network's performance.\n",
    "\n",
    "Some common loss functions include:\n",
    "\n",
    "- Mean squared error (MSE)\n",
    "- Mean absolute error (MAE)\n",
    "- Cross-entropy loss\n",
    "- Huber loss\n",
    "\n",
    "The choice of loss function depends on the specific problem and the type of output the neural network produces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60bc235",
   "metadata": {},
   "source": [
    "### Optimization Algorithms\n",
    "\n",
    "Optimization algorithms are used to adjust the weights and biases of the neural network to minimize the loss function. They play a crucial role in the training process and can significantly impact the network's performance and convergence speed.\n",
    "\n",
    "Some popular optimization algorithms include:\n",
    "\n",
    "- Stochastic gradient descent (SGD)\n",
    "- Momentum\n",
    "- Nesterov accelerated gradient (NAG)\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam\n",
    "\n",
    "Different optimization algorithms have their strengths and weaknesses, and the choice of the algorithm can be problem-dependent. Experimenting with different optimizers can help find the best one for a specific task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147315b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:06:02.912619Z",
     "start_time": "2023-04-16T19:06:02.892733Z"
    }
   },
   "source": [
    "Now that you've understood neural networks, you're ready to explore the world of deep reinforcement learning. In the next section, we'll introduce the powerful combination of deep learning and reinforcement learning techniques and their applications in finance.\n",
    "\n",
    "Proceed to Section 2.2: **Convolutional Neural Networks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd92516",
   "metadata": {},
   "source": [
    "## 2.2 Convolutional Neural Networks\n",
    "\n",
    "Let's take a moment to explore another essential deep learning technique: Convolutional Neural Networks (CNNs). While not directly related to finance, understanding the power and capabilities of CNNs will help you better appreciate the versatility of deep learning techniques.\n",
    "\n",
    "CNNs are a type of neural network specifically designed for processing grid-like data, such as images. They are highly effective at detecting and learning spatial hierarchies in the data, which makes them an excellent choice for various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778a71e",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "Convolutional layers are the core building blocks of CNNs. They perform a mathematical operation called convolution, which involves sliding a filter or kernel across the input data and computing element-wise multiplications and summations. This process helps the network learn spatial features and patterns in the data.\n",
    "\n",
    "Convolutional layers have several hyperparameters:\n",
    "\n",
    "- **Number of filters**: The number of distinct filters or kernels applied to the input data. Each filter learns a different feature or pattern.\n",
    "- **Filter size**: The dimensions of the filters (e.g., 3x3, 5x5). Smaller filters capture finer details, while larger filters capture more global features.\n",
    "- **Stride**: The step size of the filters as they slide across the input data. A larger stride reduces the spatial dimensions of the output, resulting in a smaller feature map.\n",
    "- **Padding**: The amount of zero-padding applied to the input data's edges. Padding can help preserve the spatial dimensions of the input data and control the output size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079b128",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "Pooling layers are used in CNNs to reduce the spatial dimensions of the feature maps, which helps reduce computational complexity and control overfitting. They perform a downsampling operation by selecting a representative value from a local region of the input data.\n",
    "\n",
    "There are several types of pooling operations:\n",
    "\n",
    "- **Max-pooling**: Selects the maximum value from the local region.\n",
    "- **Average-pooling**: Calculates the average value from the local region.\n",
    "- **Global average-pooling**: Computes the average value for the entire feature map, resulting in a single value per feature map.\n",
    "\n",
    "Pooling layers have hyperparameters, such as the pooling size and stride, which determine the level of downsampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422b6e4",
   "metadata": {},
   "source": [
    "### Fully Connected Layers\n",
    "\n",
    "After the convolutional and pooling layers have extracted spatial features from the input data, the feature maps are typically flattened and passed through one or more fully connected layers. These layers perform traditional neural network operations to learn non-linear combinations of the extracted features and produce the final output.\n",
    "\n",
    "In a classification task, the output layer often uses a softmax activation function to produce class probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214f61b",
   "metadata": {},
   "source": [
    "With a deeper understanding of Convolutional Neural Networks, you are now ready to venture into the realm of deep reinforcement learning, where the power of deep learning techniques is combined with reinforcement learning to tackle complex problems.\n",
    "\n",
    "Proceed to Section 2.3: **Deep Reinforcement Learning: Unleashing the Power Within**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e1e248",
   "metadata": {},
   "source": [
    "## 2.3 Deep Reinforcement Learning\n",
    "\n",
    "Now that you've explored both reinforcement learning and deep learning techniques, it's time to witness their powerful synergy. Deep Reinforcement Learning combines the strengths of deep learning and reinforcement learning, enabling agents to learn directly from raw input data and make decisions in highly complex environments.\n",
    "\n",
    "### DQN: Deep Q-Networks\n",
    "\n",
    "Deep Q-Networks (DQNs) are a groundbreaking DRL approach introduced by researchers at DeepMind in 2013. DQNs combine Q-learning with deep neural networks, allowing agents to learn directly from high-dimensional input data, such as images or market data.\n",
    "\n",
    "DQNs use a neural network to approximate the Q-function, mapping state-action pairs to Q-values. During training, the network learns to minimize the difference between its Q-value predictions and the target Q-values, which are derived from the Bellman equation.\n",
    "\n",
    "Key improvements and techniques in DQN:\n",
    "\n",
    "- **Experience Replay**: DQNs use a replay buffer to store experiences (state, action, reward, next state) during training. Experiences are sampled randomly from the buffer and used to train the network, breaking the correlation between consecutive experiences and stabilizing learning.\n",
    "- **Target Network**: DQNs employ a separate target network to compute target Q-values. This network has the same architecture as the main network but is updated less frequently, further stabilizing learning.\n",
    "\n",
    "### Policy Gradient Methods: Better Policies\n",
    "\n",
    "Policy gradient methods are a family of DRL algorithms that directly optimize the policy by following the gradient of the expected return. Instead of learning a value function, policy gradient methods learn a stochastic policy, which maps states to probability distributions over actions.\n",
    "\n",
    "Some popular policy gradient algorithms include:\n",
    "\n",
    "- REINFORCE\n",
    "- Actor-Critic methods\n",
    "- Advantage Actor-Critic (A2C)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "\n",
    "Policy gradient methods offer several benefits over value-based methods, such as the ability to handle continuous action spaces and better exploration due to inherent stochasticity.\n",
    "\n",
    "### DRL in Finance\n",
    "\n",
    "Deep reinforcement learning has opened up a world of possibilities for algorithmic trading and finance. By using DRL, traders and investors can develop sophisticated trading strategies that learn directly from raw market data and adapt to changing market conditions.\n",
    "\n",
    "Some potential applications of DRL in finance include:\n",
    "\n",
    "- Portfolio optimization\n",
    "- Trade execution and order routing\n",
    "- Market making and liquidity provision\n",
    "- Trading signal generation\n",
    "\n",
    "As you go further into this course, you'll gain hands-on experience in applying DRL techniques to real-world financial problems and witness the power of deep reinforcement learning in action.\n",
    "\n",
    "Proceed to Section 2.4: **Exercises: Deep Reinforcement Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b762ee2",
   "metadata": {},
   "source": [
    "## 2.4 Exercises: Deep Reinforcement Learning\n",
    "\n",
    "Now that you've learned about deep reinforcement learning techniques and their potential applications in finance, it's time to test your knowledge and understanding. Solve the following exercises to sharpen your DRL skills:\n",
    "\n",
    "1. **Deep Q-Networks**: Explain the main differences between a vanilla Q-learning algorithm and a Deep Q-Network (DQN). Why is experience replay important for training a DQN?\n",
    "\n",
    "2. **Policy Gradient Methods**: In the context of policy gradient methods, what is the difference between on-policy and off-policy algorithms? Provide an example of each.\n",
    "\n",
    "3. **DRL in Finance**: Imagine you are designing a DRL-based trading algorithm. What type of deep learning architecture would you choose for processing financial time series data, and why?\n",
    "\n",
    "4. **Exploration vs. Exploitation**: In DRL, balancing exploration and exploitation is crucial. Name two techniques for managing the exploration-exploitation trade-off in DRL algorithms.\n",
    "\n",
    "Don't forget to check your answers in the next section!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e82b09",
   "metadata": {},
   "source": [
    "## 2.5 Answers: Validate your DRL Knowledge\n",
    "\n",
    "Let's review the answers to the exercises:\n",
    "\n",
    "1. **Deep Q-Networks**: The main difference between a vanilla Q-learning algorithm and a DQN is that the latter uses a deep neural network to approximate the Q-function, allowing it to learn directly from high-dimensional input data. Experience replay is important for training a DQN because it breaks the correlation between consecutive experiences by sampling randomly from a replay buffer, stabilizing learning and improving convergence.\n",
    "\n",
    "2. **Policy Gradient Methods**: On-policy algorithms learn from the same policy they are trying to optimize, while off-policy algorithms learn from a different policy (the behavior policy) than the one they are trying to optimize (the target policy). Examples: REINFORCE is an on-policy method, while Q-learning is an off-policy method.\n",
    "\n",
    "3. **DRL in Finance**: For processing financial time series data, a popular choice would be a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), such as LSTMs or GRUs. CNNs can capture local patterns and features in the time series data, while RNNs can model temporal dependencies and long-term relationships.\n",
    "\n",
    "4. **Exploration vs. Exploitation**: Two techniques for managing the exploration-exploitation trade-off in DRL algorithms are ε-greedy exploration and Boltzmann exploration. In ε-greedy exploration, the agent chooses a random action with probability ε and the best-known action with probability 1-ε. In Boltzmann exploration, the agent selects actions according to a probability distribution derived from the Q-values and a temperature parameter.\n",
    "\n",
    "Congratulations! You have successfully completed the exercises and are well on your way to understanding deep reinforcement learning. Continue your journey to the next module to learn about advanced DRL techniques and their applications in finance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde3178",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Part 3: Financial Markets with DRL\n",
    "\n",
    "In this module, we will explore the application of deep reinforcement learning (DRL) techniques to the world of finance. You will learn how to develop DRL-based trading strategies, optimize your portfolio, and navigate the complex landscape of financial markets.\n",
    "\n",
    "## 3.1 Building the Ultimate Trading Agent\n",
    "\n",
    "The goal of a DRL-based trading agent is to make profitable trading decisions by learning from historical market data and adapting to changing market conditions. In this section, we will discuss the key components and design considerations for building a successful DRL trading agent.\n",
    "\n",
    "### State Representation: Decoding Markes\n",
    "\n",
    "The first step in building a DRL trading agent is to define the state representation, which is the input to the agent's policy or value function. The state should capture relevant information about the market, such as historical price data, technical indicators, and macroeconomic variables.\n",
    "\n",
    "Some popular choices for state representation in DRL-based trading agents include:\n",
    "\n",
    "- Raw price data: The agent receives historical price data as input and learns to extract meaningful features and patterns.\n",
    "- Technical indicators: The agent is provided with precomputed technical indicators, such as moving averages, RSI, or MACD, to aid its decision-making.\n",
    "- Embeddings: The agent learns a compact representation of the market data using techniques such as autoencoders or transformers.\n",
    "\n",
    "### Action Space: Trading\n",
    "\n",
    "The action space defines the set of possible actions the agent can take at each time step. In the context of trading, the action space might include:\n",
    "\n",
    "- Discrete actions: Buy, sell, or hold a fixed number of shares or contracts.\n",
    "- Continuous actions: Allocate a percentage of the portfolio to each asset or adjust the position size.\n",
    "- Multi-asset actions: Trade multiple assets simultaneously or manage a portfolio of assets.\n",
    "\n",
    "Designing the action space is crucial, as it will determine the agent's ability to execute complex trading strategies and respond to changing market conditions.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "The reward function is the compass that guides the agent's learning process. It should provide meaningful feedback that encourages the agent to make profitable trading decisions and penalize undesirable actions.\n",
    "\n",
    "Some considerations when designing a reward function for a DRL trading agent include:\n",
    "\n",
    "- Profit and loss: The agent should be rewarded for making profitable trades and penalized for losses.\n",
    "- Transaction costs: The agent should be discouraged from excessive trading by considering transaction costs, such as fees and slippage.\n",
    "- Risk-adjusted performance: The agent should be incentivized to maximize risk-adjusted performance metrics, such as the Sharpe ratio or Sortino ratio.\n",
    "\n",
    "Proceed to Section 3.2: **The Alchemist's Toolbox: DRL Algorithms for Finance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024c242",
   "metadata": {},
   "source": [
    "## 3.2 DRL Algorithms for Finance\n",
    "\n",
    "In this section, we'll examine some popular deep reinforcement learning (DRL) algorithms and discuss their suitability for financial applications. You'll learn about the strengths and weaknesses of each algorithm and gain insights into selecting the best DRL technique for your specific financial problem.\n",
    "\n",
    "### DQN: The Deep Q-Network Pioneer\n",
    "\n",
    "As discussed in Module 2, Deep Q-Networks (DQNs) combine Q-learning with deep neural networks to learn directly from high-dimensional input data. DQNs have shown success in various domains, including finance.\n",
    "\n",
    "Strengths of DQNs in finance:\n",
    "\n",
    "- Scalable to high-dimensional input data, such as raw price data or a large set of technical indicators.\n",
    "- Stable learning through experience replay and target network techniques.\n",
    "\n",
    "Weaknesses of DQNs in finance:\n",
    "\n",
    "- Limited to discrete action spaces, making it challenging to handle continuous actions, such as position sizing or portfolio allocation.\n",
    "- May struggle with the non-stationary nature of financial markets due to the underlying Q-learning algorithm.\n",
    "\n",
    "### Policy Gradient Methods\n",
    "\n",
    "Policy gradient methods, such as REINFORCE, A2C, and PPO, directly optimize the policy by following the gradient of the expected return. These algorithms offer several benefits for financial applications:\n",
    "\n",
    "Strengths of policy gradient methods in finance:\n",
    "\n",
    "- Can handle continuous action spaces, making them suitable for position sizing and portfolio optimization problems.\n",
    "- Inherent stochasticity can improve exploration and prevent the agent from getting stuck in local optima.\n",
    "\n",
    "Weaknesses of policy gradient methods in finance:\n",
    "\n",
    "- May require more samples to learn effectively, especially for high-dimensional state spaces.\n",
    "- On-policy methods, such as REINFORCE, may struggle with the non-stationary nature of financial markets due to their reliance on the current policy.\n",
    "\n",
    "### Actor-Critic Methods: The Best of Both Worlds\n",
    "\n",
    "Actor-critic methods, such as A2C, A3C, and DDPG, combine elements of both value-based and policy-based algorithms. They use a separate \"critic\" network to estimate the value function, which helps reduce the variance of the policy gradient estimates and improve learning efficiency.\n",
    "\n",
    "Strengths of actor-critic methods in finance:\n",
    "\n",
    "- Can handle both discrete and continuous action spaces.\n",
    "- Better sample efficiency compared to pure policy gradient methods.\n",
    "- Off-policy methods, such as DDPG, can better handle the non-stationary nature of financial markets.\n",
    "\n",
    "Weaknesses of actor-critic methods in finance:\n",
    "\n",
    "- May be more challenging to implement and tune due to the presence of two networks (actor and critic).\n",
    "- Learning stability can be sensitive to hyperparameter settings.\n",
    "\n",
    "Armed with this knowledge, you can now choose the most appropriate DRL algorithm for your financial problem and begin your journey towards conquering the financial markets.\n",
    "\n",
    "Proceed to Section 3.3: **Implementing a DRL Trading Agent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53ee5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:21:51.056440Z",
     "start_time": "2023-04-16T19:21:51.038232Z"
    }
   },
   "source": [
    "## 3.3 Implementing a DRL Trading Agent\n",
    "\n",
    "In this section, we will provide a high-level overview of how to implement a deep reinforcement learning (DRL) trading agent. You will learn about the key steps involved in the process and gain insights into the practical aspects of building a DRL trading agent.\n",
    "\n",
    "### Step 1: Data Preparation\n",
    "\n",
    "Before you can train your DRL trading agent, you'll need to collect and preprocess historical market data. This might include price data, technical indicators, and macroeconomic variables. Make sure to clean and preprocess the data to remove any inconsistencies or missing values.\n",
    "\n",
    "### Step 2: State Representation and Action Space\n",
    "\n",
    "Design the state representation and action space for your trading agent, as discussed in Section 3.1. Consider the relevant market information and the complexity of the trading strategies you want your agent to learn.\n",
    "\n",
    "### Step 3: Algorithm Selection\n",
    "\n",
    "Choose a DRL algorithm that is suitable for your problem, based on the insights provided in Section 3.2. Consider factors such as the action space, state representation, and the non-stationary nature of financial markets.\n",
    "\n",
    "### Step 4: Model Implementation\n",
    "\n",
    "Implement the chosen DRL algorithm using a deep learning framework such as TensorFlow or PyTorch. Define the neural network architecture for your agent and the necessary components, such as the replay buffer or target network, depending on the algorithm.\n",
    "\n",
    "### Step 5: Reward Function and Training\n",
    "\n",
    "Design a reward function that guides your agent towards profitable trading decisions and penalizes undesirable actions, as discussed in Section 3.1. Train your agent using historical market data and monitor its performance during training. Adjust the hyperparameters and reward function as needed to improve learning.\n",
    "\n",
    "### Step 6: Evaluation and Backtesting\n",
    "\n",
    "Evaluate the performance of your trained DRL trading agent using out-of-sample market data. Backtest your agent's trading decisions and analyze its risk-adjusted performance metrics, such as the Sharpe ratio or Sortino ratio. Ensure that your agent's performance is robust and not the result of overfitting or lookahead bias.\n",
    "\n",
    "### Step 7: Refinement and Deployment\n",
    "\n",
    "Refine your DRL trading agent based on the evaluation results and iterate through the previous steps as needed. Once you are satisfied with its performance, deploy your agent in a live trading environment with proper risk management and monitoring tools in place.\n",
    "\n",
    "Congratulations! You are now equipped with the knowledge and tools necessary to build a DRL trading agent that can conquer the financial markets.\n",
    "\n",
    "Proceed to Section 3.4: **Exercises: Unleash your Inner DRL Trader**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0efd73",
   "metadata": {},
   "source": [
    "## 3.4 Exercises\n",
    "\n",
    "Now that you've learned about the key components of DRL trading agents and the steps involved in implementing them, it's time to put your knowledge to the test. Solve the following exercises to demonstrate your understanding of DRL in finance:\n",
    "\n",
    "1. **Data Preparation**: Describe the process of preparing historical market data for training a DRL trading agent. What are some common issues to watch out for?\n",
    "\n",
    "2. **State and Action Spaces**: Design a state representation and action space for a DRL trading agent that trades a portfolio of cryptocurrencies. Explain your choices and their implications.\n",
    "\n",
    "3. **DRL Algorithm Selection**: Based on your understanding of DRL algorithms from Section 3.2, which algorithm would you choose for the cryptocurrency trading agent in question 2, and why?\n",
    "\n",
    "4. **Evaluation and Backtesting**: Explain the importance of evaluating and backtesting a DRL trading agent using out-of-sample data. What are some potential pitfalls to avoid?\n",
    "\n",
    "Don't forget to check your answers in the next section!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f4070",
   "metadata": {},
   "source": [
    "## 3.5 Answers\n",
    "Let's review the answers to the exercises:\n",
    "\n",
    "1. **Data Preparation**: Preparing historical market data for training a DRL trading agent involves collecting and cleaning the data, removing inconsistencies, and filling in missing values. Some common issues to watch out for include missing or duplicate data, data errors, and lookahead bias.\n",
    "\n",
    "2. **State and Action Spaces**: For a DRL trading agent that trades a portfolio of cryptocurrencies, a possible state representation could include historical price data, technical indicators, and market sentiment features (e.g., social media or news sentiment). The action space could be continuous, representing the percentage allocation of the portfolio to each cryptocurrency. The choices of state and action spaces will influence the complexity of the trading strategies the agent can learn and its ability to respond to changing market conditions.\n",
    "\n",
    "3. **DRL Algorithm Selection**: For the cryptocurrency trading agent in question 2, an actor-critic method such as DDPG or PPO might be a good choice, as these algorithms can handle continuous action spaces and provide better sample efficiency compared to pure policy gradient methods. Additionally, off-policy methods like DDPG may be better suited to handle the non-stationary nature of financial markets.\n",
    "\n",
    "4. **Evaluation and Backtesting**: Evaluating and backtesting a DRL trading agent using out-of-sample data is important to ensure its performance is robust and not the result of overfitting or lookahead bias. Potential pitfalls to avoid include overfitting, lookahead bias, and using unrealistic assumptions about transaction costs or liquidity.\n",
    "\n",
    "Congratulations! You have successfully completed the exercises and demonstrated your expertise in DRL trading. Continue your journey to the next module to learn about the intricacies of DRL-based portfolio optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec1c16",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Part 4: Portfolio Optimization and Risk Management: The DRL Way\n",
    "\n",
    "In this module, we will dive into the world of portfolio optimization and risk management using deep reinforcement learning (DRL). You will learn about the challenges of optimizing portfolios and managing risk in financial markets and discover how DRL techniques can help you overcome these obstacles.\n",
    "\n",
    "## 4.1 Modern Portfolio Theory: The Classic Approach\n",
    "\n",
    "Before we go into DRL-based portfolio optimization, let's briefly review the foundation of modern portfolio theory (MPT). Developed by Harry Markowitz in the 1950s, MPT provides a mathematical framework for assembling a portfolio of assets that maximizes expected return for a given level of risk.\n",
    "\n",
    "The key concepts of MPT include:\n",
    "\n",
    "- Expected return: The average return of a portfolio, calculated as the weighted sum of the expected returns of the individual assets.\n",
    "- Portfolio risk: The total risk of a portfolio, measured as the standard deviation of its returns. MPT assumes that risk can be reduced through diversification, as assets with low or negative correlation can offset each other's volatility.\n",
    "- Efficient frontier: The set of optimal portfolios that offer the highest expected return for a given level of risk or the lowest risk for a given level of expected return. Investors can use the efficient frontier to choose the portfolio that best aligns with their risk tolerance and return objectives.\n",
    "\n",
    "MPT has been widely adopted in finance, but it also has limitations, such as its reliance on historical data, the assumption of normally distributed returns, and the challenges of accurately estimating asset correlations.\n",
    "\n",
    "Proceed to Section 4.2: **DRL to the Rescue: Overcoming MPT Limitations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d075230",
   "metadata": {},
   "source": [
    "## 4.2 DRL: Overcoming MPT Limitations\n",
    "\n",
    "Deep reinforcement learning offers a promising alternative to modern portfolio theory (MPT) by addressing some of its limitations. In this section, we'll explore how DRL can be used for portfolio optimization and risk management, and discuss its advantages over traditional MPT approaches.\n",
    "\n",
    "### Nonlinear Relationships and Complex Market Dynamics\n",
    "\n",
    "DRL can capture complex, nonlinear relationships between assets and market factors that may be difficult to model using traditional MPT techniques. By leveraging the power of deep neural networks, DRL can learn intricate market dynamics and adapt to changing market conditions more effectively than MPT.\n",
    "\n",
    "### Continuous Action Spaces\n",
    "\n",
    "DRL algorithms, such as policy gradients and actor-critic methods, can handle continuous action spaces, allowing for more fine-grained portfolio allocation decisions compared to the discrete action spaces used in some traditional optimization methods.\n",
    "\n",
    "### Risk-sensitive Exploration and Exploitation\n",
    "\n",
    "DRL agents can balance exploration and exploitation while considering risk constraints. By incorporating risk-sensitive exploration strategies and reward functions, DRL agents can learn to manage risk effectively and make better trade-offs between risk and return.\n",
    "\n",
    "### Online Learning and Adaptation\n",
    "\n",
    "DRL algorithms can learn and adapt online as new market data becomes available, allowing them to respond more quickly to changing market conditions. This is a significant advantage over traditional MPT techniques that rely on historical data and may not adapt well to new market regimes.\n",
    "\n",
    "While DRL offers many benefits for portfolio optimization and risk management, it also has its challenges, such as the need for large amounts of data, the complexity of implementing and tuning DRL algorithms, and the risk of overfitting.\n",
    "\n",
    "Proceed to Section 4.3: **Crafting a DRL Portfolio Optimization Agent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77fb0a",
   "metadata": {},
   "source": [
    "## 4.3 A DRL Portfolio Optimization Agent\n",
    "\n",
    "In this section, we'll discuss the key components of a DRL portfolio optimization agent and provide a high-level overview of the implementation process.\n",
    "\n",
    "### State Representation\n",
    "\n",
    "The state representation for a portfolio optimization agent should capture relevant market information and asset characteristics. Possible features include historical price data, technical indicators, macroeconomic variables, and market sentiment data. The choice of state representation will influence the agent's ability to learn complex market dynamics and make informed allocation decisions.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action space for a portfolio optimization agent typically represents the allocation of the portfolio to different assets. It can be either discrete, with a fixed number of allocation options, or continuous, allowing for more fine-grained allocation decisions. Continuous action spaces can be handled by DRL algorithms such as policy gradients or actor-critic methods.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "The reward function for a portfolio optimization agent should encourage both high returns and effective risk management. Common reward functions include absolute or risk-adjusted portfolio returns, such as the Sharpe ratio or Sortino ratio. The reward function should be carefully designed to avoid promoting overly risky behavior or short-term trading strategies.\n",
    "\n",
    "### DRL Algorithm Selection\n",
    "\n",
    "Select a DRL algorithm that is suitable for the portfolio optimization problem, considering factors such as the state representation, action space, and non-stationary nature of financial markets. Some popular choices include DDPG, PPO, and SAC, which can handle continuous action spaces and offer better sample efficiency compared to pure policy gradient methods.\n",
    "\n",
    "### Model Implementation and Training\n",
    "\n",
    "Implement the chosen DRL algorithm using a deep learning framework like TensorFlow or PyTorch. Define the neural network architecture for the agent and train it using historical market data. Monitor the agent's performance during training and adjust hyperparameters and reward functions as needed to improve learning.\n",
    "\n",
    "### Evaluation and Refinement\n",
    "\n",
    "Evaluate the performance of the trained DRL portfolio optimization agent using out-of-sample market data. Analyze its risk-adjusted performance metrics and ensure that its performance is robust and not the result of overfitting or lookahead bias. Refine the agent and iterate through the previous steps as needed to improve its performance.\n",
    "\n",
    "By following these steps, you can create a DRL portfolio optimization agent that is capable of navigating the complex world of financial markets and making intelligent allocation decisions.\n",
    "\n",
    "Proceed to Section 4.4: **Exercises: Become a DRL Portfolio Mastermind**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50675f1f",
   "metadata": {},
   "source": [
    "## 4.4 Exercises: DRL Portfolio\n",
    "\n",
    "Test your understanding of DRL-based portfolio optimization and risk management by solving the following exercises:\n",
    "\n",
    "1. **State Representation**: Design a state representation for a DRL portfolio optimization agent that trades a portfolio of stocks, bonds, and commodities. Explain your choices and their implications for the agent's learning capabilities.\n",
    "\n",
    "2. **Action Space**: Discuss the advantages and disadvantages of using a continuous action space for a DRL portfolio optimization agent. How do continuous action spaces influence the choice of DRL algorithm?\n",
    "\n",
    "3. **Reward Function**: Propose a reward function for a DRL portfolio optimization agent that balances return objectives with risk management considerations. Explain how this reward function can guide the agent towards intelligent allocation decisions.\n",
    "\n",
    "4. **DRL Algorithm Selection**: Based on your understanding of DRL algorithms, which algorithm would you choose for the portfolio optimization agent described in question 1, and why?\n",
    "\n",
    "Check your answers in the next section!\n",
    "\n",
    "## 4.5 Answers: Validate your DRL Portfolio Knowledge\n",
    "\n",
    "Let's review the answers to the exercises:\n",
    "\n",
    "1. **State Representation**: A possible state representation for a DRL portfolio optimization agent that trades stocks, bonds, and commodities could include historical price data, technical indicators, macroeconomic variables (e.g., interest rates, inflation, and GDP growth), and market sentiment data. The choice of state representation will influence the agent's ability to learn complex market dynamics and make informed allocation decisions.\n",
    "\n",
    "2. **Action Space**: Continuous action spaces offer several advantages for DRL portfolio optimization agents, such as more fine-grained allocation decisions and compatibility with powerful DRL algorithms like policy gradients or actor-critic methods. However, continuous action spaces can also increase the complexity of the learning problem and require more sophisticated exploration strategies.\n",
    "\n",
    "3. **Reward Function**: A possible reward function for a DRL portfolio optimization agent that balances return objectives with risk management considerations could be the risk-adjusted portfolio return, such as the Sharpe ratio (expected return minus the risk-free rate divided by the portfolio's standard deviation) or the Sortino ratio (expected return minus the risk-free rate divided by the downside risk). This reward function can guide the agent towards intelligent allocation decisions by promoting high returns while penalizing excessive risk.\n",
    "\n",
    "4. **DRL Algorithm Selection**: For the portfolio optimization agent described in question 1, an actor-critic method such as DDPG, PPO, or SAC might be a good choice, as these algorithms can handle continuous action spaces and provide better sample efficiency compared to pure policy gradient methods. Additionally, off-policy methods like DDPG may be better suited to handle the non-stationary nature of financial markets.\n",
    "\n",
    "Congratulations! You have successfully completed the exercises and demonstrated your knowledge of DRL-based portfolio optimization and risk management.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.386px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
