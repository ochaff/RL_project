{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50416231",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c381e8",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04e5cd",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872d381",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d14fba",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb2fc7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed7a00",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5bda2",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#\" data-toc-modified-id=\"-1\"></a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-2\"></a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-3\"></a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-4\"></a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-5\"></a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-6\"></a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-7\"></a></span></li><li><span><a href=\"#Project:-Creating-an-optimal-trading-strategy-using-DRL\" data-toc-modified-id=\"Project:-Creating-an-optimal-trading-strategy-using-DRL-8\">Project: Creating an optimal trading strategy using DRL</a></span></li><li><span><a href=\"#Do-this-first\" data-toc-modified-id=\"Do-this-first-9\">Do this first</a></span></li><li><span><a href=\"#Module-0:-State-Representation-and-Action-Space-Design\" data-toc-modified-id=\"Module-0:-State-Representation-and-Action-Space-Design-10\">Module 0: State Representation and Action Space Design</a></span></li><li><span><a href=\"#Module-1:-Trading-as-a-Markov-Decision-Process\" data-toc-modified-id=\"Module-1:-Trading-as-a-Markov-Decision-Process-11\">Module 1: Trading as a Markov Decision Process</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-Parameters-\" data-toc-modified-id=\"-Parameters--11.0.0.1\"> Parameters </a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Module-2.1:-Market-Data-Collection-and-Preparation\" data-toc-modified-id=\"Module-2.1:-Market-Data-Collection-and-Preparation-12\">Module 2.1: Market Data Collection and Preparation</a></span></li><li><span><a href=\"#Module-2.2:-Benchmark-Policy-Design\" data-toc-modified-id=\"Module-2.2:-Benchmark-Policy-Design-13\">Module 2.2: Benchmark Policy Design</a></span></li><li><span><a href=\"#Module-3.1:-RL-Algorithms-Assessment\" data-toc-modified-id=\"Module-3.1:-RL-Algorithms-Assessment-14\">Module 3.1: RL Algorithms Assessment</a></span></li><li><span><a href=\"#Module-3.2:-DRL-Algorithm-Selection-and-Implementation\" data-toc-modified-id=\"Module-3.2:-DRL-Algorithm-Selection-and-Implementation-15\">Module 3.2: DRL Algorithm Selection and Implementation</a></span></li><li><span><a href=\"#Module-4:-Reward-Function-Design-and-Agent-Training\" data-toc-modified-id=\"Module-4:-Reward-Function-Design-and-Agent-Training-16\">Module 4: Reward Function Design and Agent Training</a></span></li><li><span><a href=\"#Module-5:-Agent-Evaluation-and-Refinement\" data-toc-modified-id=\"Module-5:-Agent-Evaluation-and-Refinement-17\">Module 5: Agent Evaluation and Refinement</a></span></li><li><span><a href=\"#Module-6:-Trading-Strategy-Analysis-and-Interpretation\" data-toc-modified-id=\"Module-6:-Trading-Strategy-Analysis-and-Interpretation-18\">Module 6: Trading Strategy Analysis and Interpretation</a></span></li><li><span><a href=\"#Module-7:-Reflection-and-key-findings\" data-toc-modified-id=\"Module-7:-Reflection-and-key-findings-19\">Module 7: Reflection and key findings</a></span></li><li><span><a href=\"#Module-8:-Bonus\" data-toc-modified-id=\"Module-8:-Bonus-20\">Module 8: Bonus</a></span></li><li><span><a href=\"#Author-contribution-statement\" data-toc-modified-id=\"Author-contribution-statement-21\">Author contribution statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-author-contribution-statement:\" data-toc-modified-id=\"Example-author-contribution-statement:-21.1\">Example author contribution statement:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05e367-83f2-441c-bb09-1c3af413c4ec",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Project: Creating an optimal trading strategy using DRL\n",
    "\n",
    "Welcome to Algorithmic Trading! This notebook provides a starting point for the RL project in finance. The problem  focuses on the challenges of learning policies in sequential continuous optimization problems, learning to base decisions on real data. You may simplify or alter aspects of the original problem to make it suit your project. You will apply the knowledge you have gained throughout this course to design, implement, and evaluate a DRL agent that trades a portfolio of assets in a realistic market simulation.\n",
    "\n",
    "Your task is to create a DRL trading agent that maximizes return and manages risk in a simulated market environment. You will develop the agent from scratch, following a step-by-step process that mirrors the stages of creating a real-world DRL trading system. \n",
    "With reinforcement learning, you can learn the impact of trading decisions made today on rewards tomorrow. \n",
    "In this project, you will deal with the three curses of dimensionality for a typical finance problem, enabling to solve large sequential decision-problems within reasonable time.\n",
    "\n",
    "Some things to keep in mind while working through this project:\n",
    "* Please have a look at the tutorial ('Reinforcement Learning Tutorial Algorithmic Trading') in parallel with this project, and check the source code provided in 'Reinforcement Learning Coding Tutorial'.\n",
    "* Ensure to write an independently readable report. Hints, assignments etc. can be removed or replaced with your own text and documentation. Both code and report should look clear and consistent.\n",
    "* Pay attention to aspects such as assumptions, motivation, explanation and validation.\n",
    "* While coding, make use of print statements, assertions and test conditions\n",
    "\n",
    "**[Insert group number and names of group members]**<br>\n",
    "**Group name**<br>\n",
    "**Member 1 (s000000)**<br>\n",
    "**Member 2 (s000000)**<br>\n",
    "**Member 3 (s000000)**<br>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://projectnile.in/wp-content/uploads/2022/01/Algorithmic-Trading.jpeg\" alt=\"Algorithmic trading\" style=\"width:70%; border:0;\">\n",
    "</p>\n",
    "\n",
    "Working with Jupyter Notebook:<br>\n",
    "[Introduction to Jupyter Notebook](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)<br>\n",
    "[UT JupyterLab wiki](https://jupyter.wiki.utwente.nl/)<br>\n",
    "[Markdown language](https://learn.microsoft.com/en-us/azure/devops/project/wiki/markdown-guidance?view=azure-devops)<br>\n",
    "[LaTeX mathematics](https://en.wikibooks.org/wiki/LaTeX/Mathematics)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc9898e-47be-41b5-bcf0-4134f2fed9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jupyter_contrib_nbextensions\n",
    "# If you use anaconda:\n",
    "#conda install -c conda-forge jupyter_contrib_nbextensions\n",
    "\n",
    "#!jupyter contrib nbextension install \n",
    "#!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb19a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T19:00:03.963812Z",
     "start_time": "2023-04-16T18:59:59.678437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for visualizations\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Enable inline plotting for matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dfaec",
   "metadata": {},
   "source": [
    "# Do this first\n",
    "Please go through the Reinforcement Learning Coding Tutorial first. For every module, you might find the relevant code there, in particular in the section on Reinforcement Learning for Trading with Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f8a5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T07:59:28.804605Z",
     "start_time": "2023-04-17T07:59:28.781628Z"
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 0: State Representation and Action Space Design\n",
    "\n",
    "Design a state representation and action space for your DRL trading agent. Consider the types of assets, the agent's trading objectives, and the market factors that might influence its decisions. Ensure that the chosen state and action spaces are suitable for the DRL algorithm you will use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d50b9c",
   "metadata": {},
   "source": [
    "# Module 1: Trading as a Markov Decision Process\n",
    "The first step of tackling the problem is to mathematically define the problem as a Markov Decision Process. Ensure to be precise and consistent in your formulations, as the mathematical framework forms the basis for both your implementation and communicating the problem with others. \n",
    "\n",
    "Here, create a very simple MDP, with some basic states, actions, rewards and an example transition function for the stock price process. Note: You need a model of how the stock price evolves over time and what the stock price in the period is. You can e.g. use next_price = current_price * np.random.normal(1, 0.01).\n",
    "    \n",
    "Include the following elements in the MDP:\n",
    "* <b>State</b> (information needed for determining (i) action, (ii) reward, (iii) transition)\n",
    "* <b>Action</b></b> (inc. constraints that filter out infeasible actions), \n",
    "* <b>Reward</b> function (direct reward/cost corresponding to state-action pair)\n",
    "* <b>Transition</b> function (deterministic transition to post-decision state, stochastic transition to next state. Some simplifications for the latter are allowed in case of complicated transitions). Specifically, the following is expected:\n",
    "    *  <b>Deterministic</b>: The deterministic part of the transition (from pre-decision state to post-decision state, based on the action), should be explicit. Later in the project, the post-decision state will be used to design features.\n",
    "    *  <b>Stochastic</b>: If not reasonably possible (e.g., when requiring advanced combinatorics), the stochastic part of the transition (describing exogenous outcomes) may be simplified. Note that for the RL algorithms, we do not need to know the probability of each scenario (i.e., each vector permutation), as we simply sample random numbers.\n",
    "    *  <b>Transition matrix</b>: For running the value iteration algorithm, it is advisable to run a toy problem with a manually inserted transition matrix. Here, you can use a very simplistic example, e.g. just 2 possible stock prices with random transitions.\n",
    "* Also add a (discounted) objective function\n",
    "    * Reflect on the role of the discount rate in this problem setting\n",
    "    * You may choose for a finite or infinite horizon variant. The infinite variant might be slightly easier to work with (why?).\n",
    "\n",
    "You can use LaTeX comments to write out the mathematics, e.g.,\n",
    "\n",
    "<h4> Parameters </h4>\n",
    "\\begin{equation}\n",
    "   \\text{Decision epoch}\\\\\n",
    "   t \\in \\mathcal{T} = \\{0,1,\\ldots,T\\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7983d-a6cc-4620-b107-8cbc2e819688",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 2.1: Market Data Collection and Preparation \n",
    "Collect and preprocess historical market data for a portfolio of assets. Clean the data, remove inconsistencies, and fill in missing values. Create a dataset that can be used for training and evaluating your DRL trading agent.\n",
    "Give a short descriptive analysis of the data (e.g. histogram of returns, summary statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51def2",
   "metadata": {},
   "source": [
    "# Module 2.2: Benchmark Policy Design\n",
    "\n",
    "You will design at least one benchmark policy. Naturally, the higher the quality of the benchmark, the better you can evaluate the performance of the RL algorithm.\n",
    "\n",
    "* Create one or more benchmark policies (e.g., a rule-based trading heuristic). You might opt for extremely simple baseline heuristics (single decision rule), but advanced solutions are assessed positively.\n",
    "* Explain the decision rules you incorporate and how they anticipate the future.\n",
    "* You can use this benchmark to evaluate the quality of the RL algorithms you design from here on.\n",
    "* Note you can define multiple benchmarks. Simple decision rules offer a sanity check and a lower bound, more advanced heuristics may be competitive with RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103a8aa-5b81-4440-a1bd-a10f452b1382",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 3.1: RL Algorithms Assessment\n",
    "Critically assess why dynamic programming and tabular Q-learning/SARSA would struggle to provide policies for the problem at hand. Explain what modifications would be needed for these methods to solve the problem, and what impact these modifications would have on the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4f9b6",
   "metadata": {},
   "source": [
    "# Module 3.2: DRL Algorithm Selection and Implementation\n",
    "Select a suitable DRL algorithm for your trading agent, taking into account the state representation, action space, and non-stationary nature of financial markets. Implement the chosen algorithm using a deep learning framework like TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ec8f2-9bc8-4ae8-b924-09211019c784",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 4: Reward Function Design and Agent Training\n",
    "\n",
    "Design a reward function that encourages your trading agent to achieve high returns while managing risk effectively. Train your agent using the prepared market data, adjusting hyperparameters and reward functions as needed to improve learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355542ed-2dcc-4749-b59d-94898cafc49c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 5: Agent Evaluation and Refinement\n",
    "\n",
    "Evaluate the performance of your trained trading agent using out-of-sample market data. Analyze its risk-adjusted performance metrics and ensure that its performance is robust and not the result of overfitting or lookahead bias. Refine the agent and iterate through the previous steps as needed to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220ad4e-1249-474f-91ed-a8a532b7218e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 6: Trading Strategy Analysis and Interpretation\n",
    "\n",
    "Analyze the trading strategies learned by your agent and interpret its decisions in the context of market dynamics and asset characteristics. Identify any interesting patterns or insights that can be gained from the agent's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e354c-514a-4eee-a15c-143050578320",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Module 7: Reflection and key findings\n",
    "Summarize your key findings and reflect on your work.\n",
    "* How well does your final model perform? Can you compare performance to other policies?\n",
    "* What discrepancies exist between your model and a real-world variant of your problem?\n",
    "* How did you address the dimensionality challenges? \n",
    "* What computational limits did you encounter?\n",
    "* What future improvement steps could be taken?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dffcae",
   "metadata": {},
   "source": [
    "# Module 8: Bonus\n",
    "Write the report using overleaf/ latex. Add all charts and follow this structure:\n",
    "- Introduction\n",
    "- Literature Review\n",
    "(very short on existing literature on deep RL trading strategies)\n",
    "- Theoretical foundations\n",
    "(very short on the algo you used, likely Deep RL. No need to explain all concepts from RL again.)\n",
    "- Data\n",
    "(very short, the data you used, plus some summary statistics)\n",
    "- Implementation\n",
    "(the exact implementation you did. Be very specific here, what parameters, how many training iterations)\n",
    "- Results\n",
    "(outcome and interpretation of results)\n",
    "- Conclusion\n",
    "(summary and next steps)\n",
    "- Bibliography \n",
    "- Appendix: Code (or link to github for the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd19c0d-8b0e-4a8f-830c-d7498f0129ec",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "For all group assignments and presentations, it holds that you should hand in/present your own and original work, in line with the Rules & Guidelines of the Examination Board. You have to add an “author contribution & use of AI statement” to the group assignment.\n",
    "\n",
    "# Author contribution statement\n",
    "The author contribution statement should include who did what (tasks) and what was the relative contribution of each group member to the overall contribution (percentage). Also, all group members should agree on the final version of the assignment.\n",
    "\n",
    "## Example author contribution statement:\n",
    "* Name group member 1: Wrote the introduction of the report, produced the mathematical model of Module 1, downloaded and cleaned the data, produced output statistics and wrote answers 1.1 and 2.3. She debugged the Python code to make the mathematical program work. She read the final version of the report and made final edits. [20%]\n",
    "* Name group member 2: … [30%]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "385.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
